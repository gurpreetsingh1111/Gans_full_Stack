{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter-1 Implements the GAN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W0ZlXSz5ORYt"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input,Dense,Reshape,Flatten\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential ,Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define input image dimensions\n",
        "#Large images take too much time and resources.\n",
        "img_rows = 28\n",
        "img_cols = 28\n",
        "channels = 1\n",
        "img_shape = (img_rows, img_cols, channels)"
      ],
      "metadata": {
        "id": "gqd4m6uMOimt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks."
      ],
      "metadata": {
        "id": "dU9htb9SOipj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator():\n",
        "\n",
        "    noise_shape = (100,) #1D array of size 100 (latent vector / noise) that generate the noise into  input of the images \n",
        "\n",
        "#Define your generator network \n",
        "#Here we are only using Dense layers. But network can be complicated based\n",
        "#on the application. For example, you can use VGG for super res. GAN.         \n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(256, input_shape=noise_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))# momentum means how fast actually to be trains \n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    \n",
        "    model.add(Dense(np.prod(img_shape), activation='tanh'))#Return the product of array elements over a given axis.\n",
        "    model.add(Reshape(img_shape))\n",
        "\n",
        "    model.summary()\n",
        "    # Noise is basically a random noise image pass to the Generators a 2D vector image\n",
        "    noise = Input(shape=noise_shape)\n",
        "    img = model(noise)    #Generated image (Fake images)\n",
        "\n",
        "    return Model(noise, img)\n",
        "#Alpha — α is a hyperparameter which controls the underlying value to which the\n",
        "#function saturates negatives network inputs.\n",
        "#Momentum — Speed up the training"
      ],
      "metadata": {
        "id": "fu8fSuJ2OisN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Given an input image, the Discriminator outputs the likelihood of the image being real.\n",
        "    #Binary classification - true or false (we're calling it validity)\n",
        "\n",
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=img_shape))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    img = Input(shape=img_shape)\n",
        "    validity = model(img)\n",
        "    return Model(img, validity)# (fake image,real image )\n",
        "#The validity is the Discriminator’s guess of input being real or not."
      ],
      "metadata": {
        "id": "YId2mh-LOiu-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now that we have constructed our two models it’s time to pit them against each other.\n",
        "#We do this by defining a training function, loading the data set, re-scaling our training\n",
        "#images and setting the ground truths. \n",
        "def train(epochs, batch_size=128, save_interval=50):\n",
        "\n",
        "    # Load the dataset(Load real images)\n",
        "    (X_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "    # Convert to float and Rescale -1 to 1 (Can also do 0 to 1)\n",
        "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "    print(X_train.shape)# (28 X 28)\n",
        "\n",
        "#Add channels dimension. As the input to our gen and discr. has a shape 28x28x1.\n",
        "    X_train = np.expand_dims(X_train, axis=3) #(28 X 28 X 1)\n",
        "    print(X_train.shape)\n",
        "    half_batch = int(batch_size / 2)# Means half of the image going of discriminator and half of the real image going to the discriminator\n",
        "\n",
        "#We then loop through a number of epochs to train our Discriminator by first selecting\n",
        "#a random batch of images from our true dataset, generating a set of images from our\n",
        "#Generator, feeding both set of images into our Discriminator, and finally setting the\n",
        "#loss parameters for both the real and fake images, as well as the combined loss. \n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "        # each epochs train discreminators\n",
        "        # Select a random half batch of real images\n",
        "        \"\"\"Generate the discriminator first then generator while we traning the discriminator then generator does nothing \n",
        "        and while the we traning the genrator then discriminator does nothing \n",
        "        \"\"\"\n",
        "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
        "        imgs = X_train[idx] \n",
        "        noise = np.random.normal(0, 1, (half_batch, 100))#(64,100)=(row,columns)\n",
        "\n",
        "        # Generate a half batch of fake images\n",
        "        gen_imgs = generator.predict(noise)\n",
        "\n",
        "        # Train the discriminator on real and fake images, separately\n",
        "        #Research showed that separate training is more effective. \n",
        "        d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
        "        d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
        "    #take average loss from real and fake images. \n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) \n",
        "\n",
        "#And within the same loop we train our Generator, by setting the input noise and\n",
        "#ultimately training the Generator to have the Discriminator label its samples as valid\n",
        "#by specifying the gradient loss.\n",
        "        # ---------------------\n",
        "        #  Train Generator\n",
        "        # ---------------------\n",
        "#Create noise vectors as input for generator. \n",
        "#Create as many noise vectors as defined by the batch size. \n",
        "#Based on normal distribution. Output will be of size (batch size, 100)\n",
        "        noise = np.random.normal(0, 1, (batch_size, 100)) \n",
        "\n",
        "        # The generator wants the discriminator to label the generated samples\n",
        "        # as valid (ones)\n",
        "        #This is where the genrator is trying to trick discriminator into believing\n",
        "        #the generated image is true (hence value of 1 for y)\n",
        "        valid_y = np.array([1] * batch_size) #Creates an array of all ones of size=batch size\n",
        "\n",
        "        # Generator is part of combined where it got directly linked with the discriminator\n",
        "        # Train the generator with noise as x and 1 as y. \n",
        "        # Again, 1 as the output as it is adversarial and if generator did a great\n",
        "        #job of folling the discriminator then the output would be 1 (true)\n",
        "        g_loss = combined.train_on_batch(noise, valid_y)\n",
        "\n",
        "\n",
        "#Additionally, in order for us to keep track of our training process, we print the\n",
        "#progress and save the sample image output depending on the epoch interval specified.  \n",
        "# Plot the progress\n",
        "        \n",
        "        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "        # If at save interval => save generated image samples\n",
        "        if epoch % save_interval == 0:\n",
        "            save_imgs(epoch)"
      ],
      "metadata": {
        "id": "s3Akl3WDOix2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_imgs(epoch):\n",
        "    r, c = 5, 5\n",
        "    noise = np.random.normal(0, 1, (r * c, 100))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(\"/content/drive/MyDrive/images/mnist_%d.png\" % epoch)\n",
        "    plt.close()\n",
        "#This function saves our images for us to view"
      ],
      "metadata": {
        "id": "ygTPKLEEOi0J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let us also define our optimizer for easy use later on.\n",
        "#That way if you change your mind, you can change it easily here\n",
        "optimizer = Adam(0.0002, 0.5)  #Learning rate and momentum.\n",
        "\n",
        "# Build and compile the discriminator first. \n",
        "#Generator will be trained as part of the combined model, later. \n",
        "#pick the loss function and the type of metric to keep track.                 \n",
        "#Binary cross entropy as we are doing prediction and it is a better\n",
        "#loss function compared to MSE or other. \n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "#build and compile our Discriminator, pick the loss function\n",
        "\n",
        "#SInce we are only generating (faking) images, let us not track any metrics.\n",
        "generator = build_generator()\n",
        "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "##This builds the Generator and defines the input noise. \n",
        "#In a GAN the Generator network takes noise z as an input to produce its images.  \n",
        "z = Input(shape=(100,))   #Our random input to the generator\n",
        "img = generator(z)\n",
        "\n",
        "#This ensures that when we combine our networks we only train the Generator.\n",
        "#While generator training we do not want discriminator weights to be adjusted. \n",
        "#This Doesn't affect the above descriminator training.     \n",
        "discriminator.trainable = False  \n",
        "\n",
        "#This specifies that our Discriminator will take the images generated by our Generator\n",
        "#and true dataset and set its output to a parameter called valid, which will indicate\n",
        "#whether the input is real or not.  \n",
        "valid = discriminator(img)  #Validity check on the generated image\n",
        "\n",
        "\n",
        "#Here we combined the models and also set our loss function and optimizer. \n",
        "#Again, we are only training the generator here. \n",
        "#The ultimate goal here is for the Generator to fool the Discriminator.  \n",
        "# The combined model  (stacked generator and discriminator) takes\n",
        "# noise as input => generates images => determines validity\n",
        "\n",
        "combined = Model(z, valid)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "\n",
        "train(epochs=100, batch_size=32, save_interval=10)\n",
        "\n",
        "#Save model for future use to generate fake images\n",
        "#Not tested yet... make sure right model is being saved..\n",
        "#Compare with GAN4\n",
        "\n",
        "generator.save('generator_model.h5')  #Test the model on GAN4_predict...\n",
        "#Change epochs back to 30K\n",
        "                \n",
        "#Epochs dictate the number of backward and forward propagations, the batch_size\n",
        "#indicates the number of training samples per backward/forward propagation, and the\n",
        "#sample_interval specifies"
      ],
      "metadata": {
        "id": "-4briqRSOi2q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6aiOJ_zsOi5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_lCBdnXuOi8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MyZQHzyaOi--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jx_zQH_kOjB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "k7iYePd9OjD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9M0l4CClOjHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "diTHEE-nOjV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GNxPHMtSOjYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CraIUf7GOjaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "J9OHFujYOjdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9lzejq94Ojhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GEDaH2jiOjlM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}